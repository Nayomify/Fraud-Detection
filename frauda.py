# -*- coding: utf-8 -*-
"""FraudA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UOp_aSmxMPuFS7tHkFDoRH4el6Xic2OJ
"""

import pandas as pd
import numpy as np
import pickle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from xgboost import XGBClassifier
import shap

# === 1. LOAD AND PREPROCESS DATA ===
df = pd.read_csv("enhanced_auto_insurance (5).csv")

# Drop unnecessary high-cardinality columns
drop_cols = [
    'Claim ID', 'Customer ID', 'Policy ID', 'Accident Date', 'Claim Date',
    'Claim Settlement Date', 'Accident Date Only', 'Accident Time Only'
]
df.drop(columns=drop_cols, inplace=True)

# Convert target
df['Is Fraud'] = df['Is Fraud'].astype(int)

# Encode categorical columns
for col in df.select_dtypes('object').columns:
    df[col] = LabelEncoder().fit_transform(df[col])

# === 2. SELECT TOP 20 FEATURES BASED ON PRIOR EXPLAINABILITY ===
top_20_features = [
    'total_claim_payout',
    'Total_Claim_Amount',
    'Repair Cost Estimate',
    'Number of Previous Claims',
    'claim_amount_premium_ratio',
    'vehicle_claim',
    'policy_annual_premium',
    'property_claim_deductable_ratio',
    'Premium Amount',
    'Deductible',
    'property_claim',
    'Coverage Amount',
    'Odometer Reading at Time of Claim',
    'Annual Income',
    'Credit Score',
    'Vehicle Value',
    'Claim Amount',
    'Policy Tenure',
    'months_as_customer',
    'Fraud Investigation Days'
]

X = df[top_20_features]
y = df['Is Fraud']

# === 3. SPLIT AND SCALE ===
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# === 4. TRAIN MODEL ===
model = XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    n_estimators=50,
    max_depth=3,
    random_state=42
)
model.fit(X_train_scaled, y_train)

# === 5. EVALUATION ===
y_pred = model.predict(X_test_scaled)
print("Classification Report:\n", classification_report(y_test, y_pred))
print("ROC AUC Score:", roc_auc_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
print("Confusion Matrix Visualization:")
ConfusionMatrixDisplay.from_predictions(y_test, y_pred)


# === 6. SAVE MODEL AND SCALER ===
with open("best_fraud_model_top20.pkl", "wb") as f:
    pickle.dump(model, f)

with open("scaler_top20.pkl", "wb") as f:
    pickle.dump(scaler, f)

# === 7. SHAP EXPLAINER (FOR STREAMLIT USAGE) ===
explainer = shap.Explainer(model, X_train_scaled)
with open("shap_explainer_top20.pkl", "wb") as f:
    pickle.dump(explainer, f)

# === 8. RISK SCORING MAPPING (DOCUMENTATION FOR STREAMLIT APP) ===
"""
Risk Score Mapping Logic (To be used in App):
    prob = model.predict_proba(user_scaled_input)[0][1]
    risk_score = int(prob * 100)

    if risk_score <= 50:
        Risk Level = "Low Risk (Recommended for Claim Payout)"
    else:
        Risk Level = "High Risk (Needs Supporting Docs / Investigation)"
"""

print("\nâœ… Model, Scaler, and SHAP Explainer Saved Successfully.")